{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### This is a template for ETL pipeline. This template contains 3 parts:\n",
        "* Data extracting (from .csv/.json/.xml/.sql/API)\n",
        "* Data transfering (cleaning/combining/datatype processing/date parsing/encoing/missing values/duplicates/outliers/scaling)\n",
        "* Data loading\n"
      ],
      "metadata": {
        "id": "uRZHS4o07022"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Extract data\n"
      ],
      "metadata": {
        "id": "St6XqWmk9ADO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract from CSV"
      ],
      "metadata": {
        "id": "jHDagnpu9G0U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-3yKmGl7bk5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_projects = pd.read_csv('projects_data.csv')\n",
        "df_projects = pd.read_csv('projects_data.csv', dtype=str)\n",
        "df_population = pd.read_csv('population_data.csv', skiprows=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('population_data.csv')\n",
        "for i in range(10):\n",
        "    line = f.readline()\n",
        "    print('line: ', i,  line)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "HtSPlIyo9lAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_projects.head()\n",
        "\n",
        "#Count the number of null values in each column\n",
        "df_projects.isnull().sum()\n",
        "\n",
        "#Sum the null values by column(in each row)\n",
        "df_population.isnull().sum(axis=1)\n",
        "\n",
        "# This code outputs any row that contains a null value\n",
        "df_population[df_population.isnull().any(axis=1)]"
      ],
      "metadata": {
        "id": "2IAmQ3o891oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_projects.shape"
      ],
      "metadata": {
        "id": "D6R_IWnv9686"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_population = df_population.drop('Unnamed: 62', axis=1)"
      ],
      "metadata": {
        "id": "BKuVtCxL-JKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract from JSON"
      ],
      "metadata": {
        "id": "YGUqzrs9-1bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_lines(n, file_name):\n",
        "    f = open(file_name)\n",
        "    for i in range(n):\n",
        "        print(f.readline())\n",
        "    f.close()\n",
        "\n",
        "print_lines(1, 'population_data.json')"
      ],
      "metadata": {
        "id": "VLrp6Tva_RBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first \"line\" in the file is actually the entire file. JSON is a compact way of representing data in a dictionary-like format. Luckily, pandas has a method to [read in a json file](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html).\n",
        "\n",
        "If you open the link with the documentation, you'll see there is an *orient* option that can handle JSON formatted in different ways:\n",
        "```\n",
        "'split' : dict like {index -> [index], columns -> [columns], data -> [values]}\n",
        "'records' : list like [{column -> value}, ... , {column -> value}]\n",
        "'index' : dict like {index -> {column -> value}}\n",
        "'columns' : dict like {column -> {index -> value}}\n",
        "'values' : just the values array\n",
        "```\n",
        "\n",
        "In this case, the JSON is formatted with a 'records' orientation, so you'll need to use that value in the read_json() method. You can tell that the format is 'records' by comparing the pattern in the documentation with the pattern in the JSON file.\n",
        "\n",
        "Next, read in the population_data.json file using pandas."
      ],
      "metadata": {
        "id": "jUgmFkOw_W5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_json = pd.read_json('population_data.json', orient='records')"
      ],
      "metadata": {
        "id": "lNXNgcdt_bab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# read in the JSON file\n",
        "with open('population_data.json') as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "# print the first record in the JSON file\n",
        "print(json_data[0])\n",
        "print('\\n')\n",
        "\n",
        "# show that JSON data is essentially a dictionary\n",
        "print(json_data[0]['Country Name'])\n",
        "print(json_data[0]['Country Code'])"
      ],
      "metadata": {
        "id": "5P_-cCzS_hAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract from XML"
      ],
      "metadata": {
        "id": "gr3tvqlA_mlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the BeautifulSoup library\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# open the population_data.xml file and load into Beautiful Soup\n",
        "with open(\"population_data.xml\") as fp:\n",
        "    soup = BeautifulSoup(fp, \"lxml\") # lxml is the Parser type\n",
        "\n",
        "# output the first 5 records in the xml file\n",
        "# this is an example of how to navigate with BeautifulSoup\n",
        "\n",
        "i = 0\n",
        "# use the find_all method to get all record tags in the document\n",
        "for record in soup.find_all('record'):\n",
        "    # use the find_all method to get all fields in each record\n",
        "    i += 1\n",
        "    for record in record.find_all('field'):\n",
        "        print(record['name'], ': ' , record.text)\n",
        "    print()\n",
        "    if i == 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "uYRvxVgd_s_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a data frame from the xml file.\n",
        "The dataframe should have the following layout:\n",
        "\n",
        "| Country or Area | Year | Item | Value |\n",
        "|----|----|----|----|\n",
        "| Aruba | 1960 | Population, total | 54211 |\n",
        "| Aruba | 1961 | Population, total | 55348 |\n",
        "etc..."
      ],
      "metadata": {
        "id": "KLmguyLUAByQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output the first 5 records in the xml file\n",
        "# this is an example of how to navigate with BeautifulSoup\n",
        "\n",
        "# use the find_all method to get all record tags in the document\n",
        "data_dictionary = {'Country or Area':[], 'Year':[], 'Item':[], 'Value':[]}\n",
        "\n",
        "for record in soup.find_all('record'):\n",
        "    for record in record.find_all('field'):\n",
        "        data_dictionary[record['name']].append(record.text)\n",
        "\n",
        "df = pd.DataFrame.from_dict(data_dictionary)\n",
        "df = df.pivot(index='Country or Area', columns='Year', values='Value')\n",
        "df.reset_index(level=0, inplace=True)"
      ],
      "metadata": {
        "id": "B-BEGbX8ABVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract from SQL Databases"
      ],
      "metadata": {
        "id": "7OagqCynAQDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo: SQLite3 and Pandas"
      ],
      "metadata": {
        "id": "tayyXw-lAcK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# connect to the database\n",
        "conn = sqlite3.connect('population_data.db')\n",
        "\n",
        "# run a query\n",
        "pd.read_sql('SELECT * FROM population_data', conn)\n",
        "pd.read_sql('SELECT \"Country_Name\", \"Country_Code\", \"1960\" FROM population_data', conn)"
      ],
      "metadata": {
        "id": "8TMEIZyGAR8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo: SQLAlchemy and Pandas\n",
        "If you are working with a different type of database such as MySQL or PostgreSQL, you can use the SQLAlchemy library with pandas. Here are the instructions for connecting to [different types of databases using SQLAlchemy](http://docs.sqlalchemy.org/en/latest/core/engines.html)."
      ],
      "metadata": {
        "id": "G6R2zr1KAmua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "engine = create_engine('sqlite:////home/workspace/3_sql_exercise/population_data.db')\n",
        "pd.read_sql(\"SELECT * FROM population_data\", engine)"
      ],
      "metadata": {
        "id": "B-tCfY8UA24s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract From APIs"
      ],
      "metadata": {
        "id": "TGji1ztmA8ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "url = 'http://api.worldbank.org/v2/countries/br;cn;us;de/indicators/SP.POP.TOTL/?format=json&per_page=1000'\n",
        "r = requests.get(url)\n",
        "r.json()"
      ],
      "metadata": {
        "id": "_NGuGL5oBE-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This json data isn't quite ready for a pandas data frame. Notice that the json response is a list with two entries. The first entry is\n",
        "```\n",
        "{'lastupdated': '2018-06-28',\n",
        "  'page': 1,\n",
        "  'pages': 1,\n",
        "  'per_page': 1000,\n",
        "  'total': 232}\n",
        "```\n",
        "\n",
        "That first entry is meta data about the results. For example, it says that there is one page returned with 232 results.\n",
        "\n",
        "The second entry is another list containing the data. This data would need some cleaning to be used in a pandas data frame. That would happen later in the transformation step of an ETL pipeline. Run the cell below to read the results into a dataframe and see what happens."
      ],
      "metadata": {
        "id": "TIe75GXaBcWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(r.json()[1])"
      ],
      "metadata": {
        "id": "GHqYuXMBBlew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SbRVhGQp0pCl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}