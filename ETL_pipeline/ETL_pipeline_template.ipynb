{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRZHS4o07022"
   },
   "source": [
    "### This is a template for ETL pipeline. This template contains 3 parts:\n",
    "* Data extracting (from .csv/.json/.xml/.sql/API)\n",
    "* Data transfering (cleaning/combining/datatype processing/date parsing/encoing/missing values/duplicates/outliers/scaling)\n",
    "* Data loading\n",
    "\n",
    "**Note**:\n",
    "* All codes are restructured from resource codes provided by Udacity \"[Data Scientist](https://learn.udacity.com/nanodegrees/nd025)\" class. All rights are reserved for Udacity.\n",
    "* This is just sample codes and cannot run and produce any meaningful results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St6XqWmk9ADO"
   },
   "source": [
    "# 1. Extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHDagnpu9G0U"
   },
   "source": [
    "## Extract from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-3yKmGl7bk5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_projects = pd.read_csv('projects_data.csv')\n",
    "df_projects = pd.read_csv('projects_data.csv', dtype=str)\n",
    "df_population = pd.read_csv('population_data.csv', skiprows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtSPlIyo9lAH"
   },
   "outputs": [],
   "source": [
    "f = open('population_data.csv')\n",
    "for i in range(10):\n",
    "    line = f.readline()\n",
    "    print('line: ', i,  line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IAmQ3o891oL"
   },
   "outputs": [],
   "source": [
    "df_projects.head()\n",
    "\n",
    "#Count the number of null values in each column\n",
    "df_projects.isnull().sum()\n",
    "\n",
    "#Sum the null values by column(in each row)\n",
    "df_population.isnull().sum(axis=1)\n",
    "\n",
    "# This code outputs any row that contains a null value\n",
    "df_population[df_population.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6R_IWnv9686"
   },
   "outputs": [],
   "source": [
    "df_projects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKuVtCxL-JKB"
   },
   "outputs": [],
   "source": [
    "df_population = df_population.drop('Unnamed: 62', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Figure out the encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodings.aliases import aliases\n",
    "\n",
    "alias_values = set(aliases.values())\n",
    "\n",
    "for encoding in set(aliases.values()):\n",
    "    try:\n",
    "        df=pd.read_csv(\"mystery.csv\", encoding=encoding)\n",
    "        print('successful', encoding)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGUqzrs9-1bd"
   },
   "source": [
    "## Extract from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLrp6Tva_RBV"
   },
   "outputs": [],
   "source": [
    "def print_lines(n, file_name):\n",
    "    f = open(file_name)\n",
    "    for i in range(n):\n",
    "        print(f.readline())\n",
    "    f.close()\n",
    "\n",
    "print_lines(1, 'population_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUgmFkOw_W5J"
   },
   "source": [
    "The first \"line\" in the file is actually the entire file. JSON is a compact way of representing data in a dictionary-like format. Luckily, pandas has a method to [read in a json file](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html).\n",
    "\n",
    "If you open the link with the documentation, you'll see there is an *orient* option that can handle JSON formatted in different ways:\n",
    "```\n",
    "'split' : dict like {index -> [index], columns -> [columns], data -> [values]}\n",
    "'records' : list like [{column -> value}, ... , {column -> value}]\n",
    "'index' : dict like {index -> {column -> value}}\n",
    "'columns' : dict like {column -> {index -> value}}\n",
    "'values' : just the values array\n",
    "```\n",
    "\n",
    "In this case, the JSON is formatted with a 'records' orientation, so you'll need to use that value in the read_json() method. You can tell that the format is 'records' by comparing the pattern in the documentation with the pattern in the JSON file.\n",
    "\n",
    "Next, read in the population_data.json file using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNXNgcdt_bab"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_json = pd.read_json('population_data.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5P_-cCzS_hAW"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# read in the JSON file\n",
    "with open('population_data.json') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# print the first record in the JSON file\n",
    "print(json_data[0])\n",
    "print('\\n')\n",
    "\n",
    "# show that JSON data is essentially a dictionary\n",
    "print(json_data[0]['Country Name'])\n",
    "print(json_data[0]['Country Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gr3tvqlA_mlE"
   },
   "source": [
    "## Extract from XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYRvxVgd_s_P"
   },
   "outputs": [],
   "source": [
    "# import the BeautifulSoup library\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# open the population_data.xml file and load into Beautiful Soup\n",
    "with open(\"population_data.xml\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\") # lxml is the Parser type\n",
    "\n",
    "# output the first 5 records in the xml file\n",
    "# this is an example of how to navigate with BeautifulSoup\n",
    "\n",
    "i = 0\n",
    "# use the find_all method to get all record tags in the document\n",
    "for record in soup.find_all('record'):\n",
    "    # use the find_all method to get all fields in each record\n",
    "    i += 1\n",
    "    for record in record.find_all('field'):\n",
    "        print(record['name'], ': ' , record.text)\n",
    "    print()\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLmguyLUAByQ"
   },
   "source": [
    "Create a data frame from the xml file.\n",
    "The dataframe should have the following layout:\n",
    "\n",
    "| Country or Area | Year | Item | Value |\n",
    "|----|----|----|----|\n",
    "| Aruba | 1960 | Population, total | 54211 |\n",
    "| Aruba | 1961 | Population, total | 55348 |\n",
    "etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-BEGbX8ABVk"
   },
   "outputs": [],
   "source": [
    "# output the first 5 records in the xml file\n",
    "# this is an example of how to navigate with BeautifulSoup\n",
    "\n",
    "# use the find_all method to get all record tags in the document\n",
    "data_dictionary = {'Country or Area':[], 'Year':[], 'Item':[], 'Value':[]}\n",
    "\n",
    "for record in soup.find_all('record'):\n",
    "    for record in record.find_all('field'):\n",
    "        data_dictionary[record['name']].append(record.text)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dictionary)\n",
    "df = df.pivot(index='Country or Area', columns='Year', values='Value')\n",
    "df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OagqCynAQDM"
   },
   "source": [
    "# Extract from SQL Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tayyXw-lAcK6"
   },
   "source": [
    "### Demo: SQLite3 and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TMEIZyGAR8S"
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# connect to the database\n",
    "conn = sqlite3.connect('population_data.db')\n",
    "\n",
    "# run a query\n",
    "pd.read_sql('SELECT * FROM population_data', conn)\n",
    "pd.read_sql('SELECT \"Country_Name\", \"Country_Code\", \"1960\" FROM population_data', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6R2zr1KAmua"
   },
   "source": [
    "### Demo: SQLAlchemy and Pandas\n",
    "If you are working with a different type of database such as MySQL or PostgreSQL, you can use the SQLAlchemy library with pandas. Here are the instructions for connecting to [different types of databases using SQLAlchemy](http://docs.sqlalchemy.org/en/latest/core/engines.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-tCfY8UA24s"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:////home/workspace/3_sql_exercise/population_data.db')\n",
    "pd.read_sql(\"SELECT * FROM population_data\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGji1ztmA8ID"
   },
   "source": [
    "## Extract From APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NGuGL5oBE-W"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'http://api.worldbank.org/v2/countries/br;cn;us;de/indicators/SP.POP.TOTL/?format=json&per_page=1000'\n",
    "r = requests.get(url)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIe75GXaBcWA"
   },
   "source": [
    "This json data isn't quite ready for a pandas data frame. Notice that the json response is a list with two entries. The first entry is\n",
    "```\n",
    "{'lastupdated': '2018-06-28',\n",
    "  'page': 1,\n",
    "  'pages': 1,\n",
    "  'per_page': 1000,\n",
    "  'total': 232}\n",
    "```\n",
    "\n",
    "That first entry is meta data about the results. For example, it says that there is one page returned with 232 results.\n",
    "\n",
    "The second entry is another list containing the data. This data would need some cleaning to be used in a pandas data frame. That would happen later in the transformation step of an ETL pipeline. Run the cell below to read the results into a dataframe and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHqYuXMBBlew"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(r.json()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbRVhGQp0pCl"
   },
   "source": [
    "## 2. Transfer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the 'Unnamed: 62' column from each data set\n",
    "df_rural.drop('Unnamed: 62', axis=1, inplace=True)\n",
    "df_electricity.drop('Unnamed: 62', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_rural, df_electricity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rural_melt = pd.melt(df_rural,\\\n",
    "                        id_vars=['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code'],\\\n",
    "                       var_name = 'Year', value_name='Rural_Value')\n",
    "df_electricity_melt = pd.melt(df_electricity,\\\n",
    "                              id_vars=['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code'],\\\n",
    "                             var_name='Year', value_name='Electricity_Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: merge the data frames together based on their common columns\n",
    "# in this case, the common columns are Country Name, Country Code, and Year\n",
    "df_merge = df_rural_melt.merge(df_electricity_melt, how='outer',\\\n",
    "                               on=['Country Name', 'Country Code', 'Year'])\n",
    "\n",
    "# TODO: sort the results by country and then by year\n",
    "df_combined = df_merge.sort_values(['Country Name', 'Year'])\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the projects data set with all columns type string\n",
    "df_projects = pd.read_csv('../data/projects_data.csv', dtype=str)\n",
    "df_projects.drop(['Unnamed: 56'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indicator[['Country Name', 'Country Code']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects['countryname'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Use the Pycountry library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects['Official Country Name'] = df_projects['countryname'].str.split(';').str.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycountry\n",
    "from pycountry import countries\n",
    "# set up the libraries and variables\n",
    "from collections import defaultdict\n",
    "country_not_found = [] # stores countries not found in the pycountry library\n",
    "project_country_abbrev_dict = defaultdict(str) # set up an empty dictionary of string values\n",
    "\n",
    "# TODO: iterate through the country names in df_projects. \n",
    "# Create a dictionary mapping the country name to the alpha_3 ISO code\n",
    "for country in df_projects['Official Country Name'].drop_duplicates().sort_values():\n",
    "    try: \n",
    "        # TODO: look up the country name in the pycountry library\n",
    "        # store the country name as the dictionary key and the ISO-3 code as the value\n",
    "        project_country_abbrev_dict[country] = countries.lookup(country).alpha_3\n",
    "    except:\n",
    "        # If the country name is not in the pycountry library, then print out the country name\n",
    "        # And store the results in the country_not_found list\n",
    "        print(country, ' not found')\n",
    "        country_not_found.append(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Making a Manual Mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to iterate through the country_not_found list and check if the country name is in the df_indicator data set\n",
    "indicator_countries = df_indicator[['Country Name', 'Country Code']].drop_duplicates().sort_values(by='Country Name')\n",
    "\n",
    "for country in country_not_found:\n",
    "    if country in indicator_countries['Country Name'].tolist():\n",
    "        print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_not_found_mapping = {'Co-operative Republic of Guyana': 'GUY',\n",
    "             'Commonwealth of Australia':'AUS',\n",
    "             'Democratic Republic of Sao Tome and Prin':'STP',\n",
    "             'Democratic Republic of the Congo':'COD'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the project_country_abbrev_dict with the country_not_found_mapping dictionary\n",
    "#Python dictionaries have a method called update(), which essentially\n",
    "# appends a dictionary to another dictionary\n",
    "project_country_abbrev_dict.update(country_not_found_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the project_country_abbrev_dict and the df_projects['Country Name'] column to make a new column\n",
    "# of the alpha-3 country codes. This new column should be called 'Country Code'.\n",
    "df_projects['Country Code'] = df_projects['Official Country Name'].apply(lambda x: project_country_abbrev_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With messy data, you might find it easier to read in everything as a string; however, you'll sometimes have to convert those strings to more appropriate data types. When you output the dtypes of a dataframe, you'll generally see these values in the results:\n",
    "* float64\n",
    "* int64\n",
    "* bool\n",
    "* datetime64\n",
    "* timedelta\n",
    "* object\n",
    "\n",
    "where timedelta is the difference between two datetimes and object is a string. As you've seen here, you sometimes need to convert data types from one type to another type. Pandas has a few different methods for converting between data types, and here are link to the documentation:\n",
    "\n",
    "* [astype](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.astype.html#pandas.DataFrame.astype)\n",
    "* [to_datetime](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.to_datetime.html#pandas.to_datetime)\n",
    "* [to_numeric](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.to_numeric.html#pandas.to_numeric)\n",
    "* [to_timedelta](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.to_timedelta.html#pandas.to_timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the totalamt column from a string to a float \n",
    "df_projects['totalamt'] = pd.to_numeric(df_projects['totalamt'].str.replace(',',\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parsing Dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "parsed_date = pd.to_datetime('January 1st, 2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_date.month\n",
    "parsed_date.year\n",
    "parsed_date.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects['boardapprovaldate'] = pd.to_datetime(df_projects['boardapprovaldate'])\n",
    "df_projects['closingdate'] = pd.to_datetime(df_projects['closingdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects['boardapprovaldate'].dt.second\n",
    "df_projects['boardapprovaldate'].dt.month\n",
    "# weekday represents the day of the week from 0 (Monday) to 6 (Sunday).\n",
    "df_projects['boardapprovaldate'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects['approvalyear'] = df_projects['boardapprovaldate'].dt.year\n",
    "df_projects['approvalday'] = df_projects['boardapprovaldate'].dt.day\n",
    "df_projects['approvalweekday'] = df_projects['boardapprovaldate'].dt.weekday\n",
    "df_projects['closingyear'] = df_projects['closingdate'].dt.year\n",
    "df_projects['closingday'] = df_projects['closingdate'].dt.day\n",
    "df_projects['closingweekday'] = df_projects['closingdate'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values: Imputing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code cell to check how many null values are in the data set\n",
    "df.isnull().sum()\n",
    "\n",
    "# output percentage of values that are missing\n",
    "100 * sector.isnull().sum() / sector.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt['GDP_filled'] = df_melt.groupby('Country Name')['GDP'].transform(lambda x: x.fillna(x.mean()))\n",
    "df_melt['GDP_ffill'] = df_melt.sort_values('year').groupby('Country Name')['GDP'].fillna(method='ffill')\n",
    "df_melt['GDP_bfill'] = df_melt.sort_values('year').groupby('Country Name')['GDP'].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Correct data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: In the sector1 variable, replace the string '!$!0' with nan\n",
    "# HINT: you can use the pandas replace() method and numpy.nan\n",
    "sector['sector1'] = sector['sector1'].replace('!$!0', np.nan)\n",
    "\n",
    "# TODO: In the sector1 variable, remove the last 10 or 11 characters from the sector1 variable.\n",
    "# HINT: There is more than one way to do this including the replace method\n",
    "# HINT: You can use a regex expression '!.+'\n",
    "# That regex expression looks for a string with an exclamation\n",
    "# point followed by one or more characters\n",
    "\n",
    "sector['sector1'] = sector['sector1'].replace('!.+', '', regex=True)\n",
    "\n",
    "# TODO: Remove the string '(Historic)' from the sector1 variable\n",
    "# HINT: You can use the replace method\n",
    "sector['sector1'] = sector['sector1'].replace('^(\\(Historic\\))', '', regex=True)\n",
    "\n",
    "print('Number of unique sectors after cleaning:', len(list(sector['sector1'].unique())))\n",
    "print('Percentage of null values after cleaning:', 100 * sector['sector1'].isnull().sum() / sector['sector1'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data frame for projects over 1 billion dollars\n",
    "# count the number of unique countries in the results\n",
    "projects[projects['totalamt'] > 1000000000]['countryname'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: find the unique dates in the republics variable\n",
    "republic_unique_dates = republics['boardapprovaldate'].unique()\n",
    "\n",
    "# TODO: find the unique dates in the yugoslavia variable\n",
    "yugoslavia_unique_dates = yugoslavia['boardapprovaldate'].unique()\n",
    "\n",
    "# TODO: make a list of the results appending one list to the other\n",
    "dates = np.append(republic_unique_dates, yugoslavia_unique_dates)\n",
    "\n",
    "# TODO: print out the dates that appeared twice in the results\n",
    "unique_dates, count = np.unique(dates, return_counts=True)\n",
    "\n",
    "for i in range(len(unique_dates)):\n",
    "    if count[i] == 2:\n",
    "        print(unique_dates[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables from the sector1_aggregates data. Put the results into a dataframe called dummies\n",
    "# Hint: Use the get_dummies method\n",
    "dummies = pd.DataFrame(pd.get_dummies(sector['sector1']))\n",
    "\n",
    "# Filter the projects data for the totalamt, the year from boardapprovaldate, and the dummy variables\n",
    "projects['year'] = projects['boardapprovaldate'].dt.year\n",
    "df = projects[['totalamt','year']]\n",
    "df_final = pd.concat([df, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the rows from the data that have Country Name values in the non_countries list\n",
    "# Store the filter results back into the df_2016 variable\n",
    "\n",
    "non_countries = ['World','High income','OECD members']\n",
    "# remove non countries from the data\n",
    "df_2016 = df_2016[~df_2016['Country Name'].isin(non_countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find country names that are in the population outliers list but not the gdp outliers list\n",
    "# HINT: Python's set() and list() methods should be helpful\n",
    "\n",
    "list(set(population_outliers['Country Name']) - set(gdp_outliers['Country Name']))\n",
    "\n",
    "# Find country names that are in the gdp outliers list but not the population outliers list\n",
    "# HINT: Python's set() and list() methods should be helpful\n",
    "\n",
    "list(set(gdp_outliers['Country Name']) - set(population_outliers['Country Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **use plot to check outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the code cell below\n",
    "\n",
    "x = list(df_2016['population'])\n",
    "y = list(df_2016['gdp'])\n",
    "text = df_2016['Country Name']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.scatter(x, y)\n",
    "plt.title('GDP vs Population')\n",
    "plt.xlabel('population')\n",
    "plt.ylabel('GDP')\n",
    "for i, txt in enumerate(text):\n",
    "    ax.annotate(txt, (x[i],y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Write a function that uses the Tukey rule to detect outliers in a dataframe column \n",
    "# and then removes that entire row from the data frame. For example, if the United States \n",
    "# is detected to be a GDP outlier, then remove the entire row of United States data.\n",
    "# The function inputs should be a data frame and a column name.\n",
    "# The output is a data_frame with the outliers eliminated\n",
    "def tukey_rule(data_frame, column_name):\n",
    "    data = data_frame[column_name]\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    max_value = Q3 + 1.5 * IQR\n",
    "    min_value = Q1 - 1.5 * IQR\n",
    "\n",
    "    return data_frame[(data_frame[column_name] < max_value) & (data_frame[column_name] > min_value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier_removed = df_2016.copy()\n",
    "\n",
    "for column in ['population', 'gdp']:\n",
    "    df_outlier_removed = tukey_rule(df_outlier_removed, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_min_max(data):\n",
    "    # TODO: Complete this function called x_min_max() \n",
    "    # The input is an array of data as an input \n",
    "    # The outputs are the minimum and maximum of that array\n",
    "    minimum = min(data)\n",
    "    maximum = max(data)\n",
    "    return minimum, maximum\n",
    "\n",
    "# this should give the result (36572611.88531479, 18624475000000.0)\n",
    "x_min_max(df_2016['gdp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, x_min, x_max):\n",
    "    # TODO: Complete this function\n",
    "    # The input is a single value \n",
    "    # The output is the normalized value\n",
    "    return (x - x_min) / (x_max - x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    # TODO: Complete the normalizer class\n",
    "    # The normalizer class receives a dataframe as its only input for initialization\n",
    "    # For example, the data frame might contain gdp and population data in two separate columns\n",
    "    # Follow the TODOs in each section\n",
    "    \n",
    "    def __init__(self, dataframe):\n",
    "        \n",
    "        # TODO: complete the init function. \n",
    "        # Assume the dataframe has an unknown number of columns like [['gdp', 'population']] \n",
    "        # iterate through each column calculating the min and max for each column\n",
    "        # append the results to the params attribute list\n",
    "        \n",
    "        # For example, take the gdp column and calculate the minimum and maximum\n",
    "        # Put these results in a list [minimum, maximum]\n",
    "        # Append the list to the params variable\n",
    "        # Then take the population column and do the same\n",
    "        \n",
    "        # HINT: You can put your x_min_max() function as part of this class and use it\n",
    "        \n",
    "        self.params = []\n",
    "\n",
    "        for column in dataframe.columns:\n",
    "            self.params.append(x_min_max(dataframe[column]))\n",
    "            def x_min_max(data):\n",
    "        # TODO: complete the x_min_max method\n",
    "        # HINT: You can use the same function defined earlier in the exercise\n",
    "        minimum = min(data)\n",
    "        maximum = max(data)\n",
    "        return minimum, maximum\n",
    "\n",
    "    def normalize_data(self, x):\n",
    "        # TODO: complete the normalize_data method\n",
    "        # The function receives a data point as an input and then outputs the normalized version\n",
    "        # For example, if an input data point of [gdp, population] were used. Then the output would\n",
    "        # be the normalized version of the [gdp, population] data point\n",
    "        # Put the results in the normalized variable defined below\n",
    "        \n",
    "        # Assume that the columns in the dataframe used to initialize an object are in the same\n",
    "        # order as this data point x\n",
    "        \n",
    "        # HINT: You cannot use the normalize_data function defined earlier in the exercise.\n",
    "        # You'll need to iterate through the individual values in the x variable        \n",
    "        # Use the params attribute where the min and max values are stored \n",
    "        normalized = []\n",
    "        for i, value in enumerate(x):\n",
    "            x_max = self.params[i][1]\n",
    "            x_min = self.params[i][0]\n",
    "            normalized.append((x[i] - x_min) / (x_max - x_min))\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_normalizer = Normalizer(df_2016[['gdp', 'population']])\n",
    "gdp_normalizer.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016['gdppercapita'] = df_2016['gdp'] / df_2016['population']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: use orient='records' to get one of the more common json formats\n",
    "# HINT: be sure to specify the name of the json file you want to create as the first input into to_json\n",
    "df_merged.to_json('countrydata.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: The to_csv() method is similar to the to_json() method.\n",
    "# HINT: If you do not want the data frame indices in your result, use index=False\n",
    "df_merged.to_csv('countrydata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# connect to the database\n",
    "# the database file will be worldbank.db\n",
    "# note that sqlite3 will create this database file if it does not exist already\n",
    "conn = sqlite3.connect('worldbank.db')\n",
    "\n",
    "# TODO: output the df_merged dataframe to a SQL table called 'merged'.\n",
    "# HINT: Use the to_sql() method\n",
    "# HINT: Use the conn variable for the connection parameter\n",
    "# HINT: You can use the if_exists parameter like if_exists='replace' to replace a table if it already exists\n",
    "\n",
    "df_merged.to_sql('merged', con = conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Extra: Combine dataset with df.melt / df.merge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: merge the data sets together according to the instructions. First, use the \n",
    "# melt method to change the formatting of each data frame so that it looks like this:\n",
    "# Country Name, Country Code, Year, Rural Value\n",
    "# Country Name, Country Code, Year, Electricity Value\n",
    "\n",
    "df_rural_melt = pd.melt(df_rural,\\\n",
    "                        id_vars=['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code'],\\\n",
    "                       var_name = 'Year', value_name='Rural_Value')\n",
    "df_electricity_melt = pd.melt(df_electricity,\\\n",
    "                              id_vars=['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code'],\\\n",
    "                             var_name='Year', value_name='Electricity_Value')\n",
    "\n",
    "# TODO: drop any columns from the data frames that aren't needed\n",
    "df_rural_melt.drop(['Indicator Name', 'Indicator Code'], axis=1, inplace=True)\n",
    "df_electricity_melt.drop(['Indicator Name', 'Indicator Code'], axis=1, inplace=True)\n",
    "\n",
    "# TODO: merge the data frames together based on their common columns\n",
    "# in this case, the common columns are Country Name, Country Code, and Year\n",
    "df_merge = df_rural_melt.merge(df_electricity_melt, how='outer',\\\n",
    "                               on=['Country Name', 'Country Code', 'Year'])\n",
    "\n",
    "# TODO: sort the results by country and then by year\n",
    "df_combined = df_merge.sort_values(['Country Name', 'Year'])\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Extra: Keep certain columns and certain rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Calculate the population sum by year for Canada,\n",
    "#       the United States, and Mexico.\n",
    "# \n",
    "keepcol = ['Country Name']\n",
    "for i in range(1960, 2018, 1):\n",
    "    keepcol.append(str(i))\n",
    "\n",
    "df_nafta = df_indicator[(df_indicator['Country Name'] == 'Canada') | \n",
    "             (df_indicator['Country Name'] == 'United States') | \n",
    "            (df_indicator['Country Name'] == 'Mexico')].iloc[:,]\n",
    "\n",
    "df_nafta.sum(axis=0)[keepcol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  output all projects for the 'Socialist Federal Republic of Yugoslavia\n",
    "projects[projects['countryname'].str.contains('Yugoslavia')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# filter the projects data set for project boardapprovaldate prior to April 27th, 1992 AND with countryname\n",
    "#  of either 'Bosnia and Herzegovina', 'Croatia', 'Kosovo', 'Macedonia', 'Serbia', or 'Slovenia'. Store the\n",
    "#  results in the republics variable\n",
    "republics = projects[(projects['boardapprovaldate'] < datetime.date(1992, 4, 27)) &\n",
    "         ((projects['countryname'].str.contains('Bosnia')) | \n",
    "         (projects['countryname'].str.contains('Croatia')) | \n",
    "         (projects['countryname'].str.contains('Kosovo')) | \n",
    "         (projects['countryname'].str.contains('Macedonia')) | \n",
    "         (projects['countryname'].str.contains('Montenegro')) | \n",
    "         (projects['countryname'].str.contains('Serbia')) | \n",
    "         (projects['countryname'].str.contains('Slovenia')))][['regionname', \n",
    "                                                                   'countryname', \n",
    "                                                                   'lendinginstr', \n",
    "                                                                   'totalamt', \n",
    "                                                                   'boardapprovaldate',\n",
    "                                                               'location', \n",
    "                                                               'GeoLocID', \n",
    "                                                               'GeoLocName',\n",
    "                                                               'Latitude', \n",
    "                                                               'Longitude', \n",
    "                                                               'Country', \n",
    "                                                               'project_name']].sort_values('boardapprovaldate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Extra: Combine different category to a big category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Create the sector1_aggregates variable\n",
    "sector.loc[:,'sector1_aggregates'] = sector['sector1']\n",
    "\n",
    "# TODO: The code above created a new variable called sector1_aggregates. \n",
    "#       Currently, sector1_aggregates has all of the same values as sector1\n",
    "#       For this task, find all the rows in sector1_aggregates with the term 'Energy' in them, \n",
    "#       For all of these rows, replace whatever is the value is with the term 'Energy'.\n",
    "#       The idea is to simplify the category names by combining various categories together.\n",
    "#       Then, do the same for the term 'Transportation\n",
    "# HINT: You can use the contains() methods. See the documentation for how to ignore case using the re library\n",
    "# HINT: You might get an error saying \"cannot index with vector containing NA / NaN values.\" \n",
    "#       Try converting NaN values to something else like False or a string\n",
    "\n",
    "sector.loc[sector['sector1_aggregates'].str.contains('Energy', re.IGNORECASE).replace(np.nan, False),'sector1_aggregates'] = 'Energy'\n",
    "sector.loc[sector['sector1_aggregates'].str.contains('Transportation', re.IGNORECASE).replace(np.nan, False),'sector1_aggregates'] = 'Transportation'\n",
    "\n",
    "print('Number of unique sectors after cleaning:', len(list(sector['sector1_aggregates'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Extra: Create plot with groupby**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TODO: Make a visualization with year on the x-axis and the sum of the totalamt columns per year on the y-axis\n",
    "# HINT: The totalamt column is currently a string with commas. For example 100,250,364. You'll need to remove the\n",
    "#         commas and convert the column to a numeric variable.\n",
    "# HINT: pandas groupby, sum, and plot methods should also be helpful\n",
    "####\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df_projects['totalamt'] = pd.to_numeric(df_projects['totalamt'].str.replace(',',''))\n",
    "\n",
    "ax = df_projects.groupby('approvalyear')['totalamt'].sum().plot(x='approvalyear', y='totalamt',\n",
    "                                                          title ='Total Amount Approved per Year')\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('amount $')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Extra: create Stacked plot with groupby**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# put the data set into long form instead of wide\n",
    "df_melt = pd.melt(df, id_vars=['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code'], var_name='year', value_name='GDP')\n",
    "\n",
    "# convert year to a date time\n",
    "df_melt['year'] = pd.to_datetime(df_melt['year'])\n",
    "\n",
    "def plot_results(column_name):\n",
    "    # plot the results for Afghanistan, Albania, and Honduras\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "    df_melt[(df_melt['Country Name'] == 'Afghanistan') | \n",
    "            (df_melt['Country Name'] == 'Albania') | \n",
    "            (df_melt['Country Name'] == 'Honduras')].groupby('Country Name').plot('year', column_name, legend=True, ax=ax)\n",
    "    ax.legend(labels=['Afghanistan', 'Albania', 'Honduras'])\n",
    "    \n",
    "plot_results('GDP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Extra: feature engineering**\n",
    "\n",
    "Write code that creates multiples of a feature. For example, if you take the 'gdp' column and an integer like 3, you want to append a new column with the square of gdp (gdp^2) and another column with the cube of gdp (gdp^3).\n",
    "\n",
    "Follow the TODOs below. These functions build on each other in the following way:\n",
    "\n",
    "create_multiples(b, k) has two inputs. The first input, b, is a floating point number. The second number, k, is an integer. The output is a list of multiples of b. For example create_multiples(3, 4) would return this list: $[3^2, 3^3, 3^4]$ or in other words $[9, 27, 81]$.\n",
    "\n",
    "Then the column_name_generator(colname, k) function outputs a list of column names. For example, column_name_generator('gdp', 4) would output a list of strings `['gdp2', 'gdp3', 'gdp4']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill out the create_multiples function.\n",
    "# The create_multiples function has two inputs. A floating point number and an integer.\n",
    "# The output is a list of multiples of the input b starting from the square of b and ending at b^k.\n",
    "\n",
    "def create_multiples(b, k):\n",
    "    \n",
    "    new_features = []\n",
    "    \n",
    "    # TODO: use a for loop to make a list of multiples of b: ie b^2, b^3, b^4, etc... until b^k\n",
    "    for i in range(2,k+1):\n",
    "        new_features.append(b ** i)\n",
    "    \n",
    "    return new_features\n",
    "\n",
    "# TODO: Fill out the column_name_generator function.\n",
    "# The function has two inputs: a string representing a column name and an integer k. \n",
    "# The 'k' variable is the same as the create_multiples function.\n",
    "# The output should be a list of column names.\n",
    "# For example if the inputs are ('gdp', 4) then the output is a list of strings ['gdp2', 'gdp3', gdp4']\n",
    "def column_name_generator(colname, k):\n",
    "    \n",
    "    col_names = []\n",
    "    for i in range(2,k+1):\n",
    "        col_names.append('{}{}'.format(colname, i))\n",
    "    return col_names\n",
    "\n",
    "# TODO: Fill out the concatenate_features function.\n",
    "# The function has three inputs. A dataframe, a column name represented by a string, and an integer representing\n",
    "# the maximum power to create when engineering features.\n",
    "\n",
    "# If the input is (df_2016, 'gdp', 3), then the output will be the df_2016 dataframe with two new columns\n",
    "# One new column will be 'gdp2' ie gdp^2, and then other column will be 'gdp3' ie gdp^3.\n",
    "\n",
    "# HINT: There may be more than one way to do this.\n",
    "# The TODOs in this section point you towards one way that works\n",
    "def concatenate_features(df, column, num_columns):\n",
    "    \n",
    "    # TODO: Use the pandas apply() method to create the new features. Inside the apply method, you\n",
    "    # can use a lambda function with the create_mtuliples function\n",
    "    new_features = df[column].apply(lambda x: create_multiples(x, num_columns))\n",
    "    \n",
    "    # TODO: Create a dataframe from the new_features variable\n",
    "    # Use the column_name_generator() function to create the column names\n",
    "    \n",
    "    # HINT: In the pd.DataFrame() method, you can specify column names inputting a list in the columns option\n",
    "    # HINT: Using new_features.tolist() might be helpful\n",
    "    new_features_df = pd.DataFrame(new_features.tolist(), columns = column_name_generator(column, num_columns))\n",
    "    \n",
    "    # TODO: concatenate the original date frame in df with the new_features_df dataframe\n",
    "    # return this concatenated dataframe\n",
    "    return pd.concat([df, new_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_features(df_2016, 'gdp', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extra: Load data to SQL with SQL command**\n",
    "\n",
    "Now, it's your turn. Use the sqlite3 library to connect to the worldbank.db database. Then: \n",
    "* Create a table, called projects, for the projects data where the primary key is the id of each project. \n",
    "* Create another table, called gdp, that contains the gdp data. \n",
    "* And create another table, called population, that contains the population data.\n",
    "\n",
    "Here is the schema for each table.\n",
    "##### projects\n",
    "\n",
    "* project_id text \n",
    "* countryname text \n",
    "* countrycode text\n",
    "* totalamt real\n",
    "* year integer\n",
    "\n",
    "project_id is the primary key\n",
    "\n",
    "##### gdp\n",
    "* countryname text\n",
    "* countrycode text\n",
    "* year integer\n",
    "* gdp real\n",
    "\n",
    "(countrycode, year) is the primary key\n",
    "\n",
    "##### population\n",
    "* countryname text\n",
    "* countrycode text\n",
    "* year integer\n",
    "* population integer\n",
    "\n",
    "(countrycode, year) is the primary key\n",
    "\n",
    "After setting up the tables, write code that inserts the data into each table. (Note that this database is not normalized. For example, countryname and countrycode are in all three tables. You could make another table with countrycode and countryname and then create a foreign key constraint in the projects, gdp, and population tables. If you'd like an extra challenge, create a country table with countryname and countrycode. Then create the other tables with foreign key constraints).\n",
    "\n",
    "Follow the TODOs in the next few code cells to finish the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the data base\n",
    "conn = sqlite3.connect('worldbank.db')\n",
    "\n",
    "# get a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# drop tables created previously to start fresh\n",
    "cur.execute(\"DROP TABLE IF EXISTS test\")\n",
    "cur.execute(\"DROP TABLE IF EXISTS indicator\")\n",
    "cur.execute(\"DROP TABLE IF EXISTS projects\")\n",
    "cur.execute(\"DROP TABLE IF EXISTS gdp\")\n",
    "cur.execute(\"DROP TABLE IF EXISTS population\")\n",
    "\n",
    "# TODO create the projects table including project_id as a primary key\n",
    "# HINT: Use cur.execute(\"SQL Query\")\n",
    "cur.execute(\"CREATE TABLE projects (project_id TEXT PRIMARY KEY, countryname TEXT, countrycode TEXT, totalamt REAL, year INTEGER);\")\n",
    "\n",
    "# TODO: create the gdp table including (countrycode, year) as primary key\n",
    "# HINT: To create a primary key on multiple columns, you can do this:\n",
    "# CREATE TABLE tablename (columna datatype, columnb datatype, columnc dataype, PRIMARY KEY (columna, columnb));\n",
    "cur.execute(\"CREATE TABLE gdp (countryname TEXT, countrycode TEXT, year INTEGER, gdp REAL, PRIMARY KEY (countrycode, year));\")\n",
    "\n",
    "# TODO: create the population table including (countrycode, year) as primary key\n",
    "cur.execute(\"CREATE TABLE population (countryname TEXT, countrycode TEXT, year INTEGER, population REAL, PRIMARY KEY (countrycode, year));\")\n",
    "\n",
    "# commit changes to the database. Do this whenever modifying a database\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:insert project values into the projects table\n",
    "# HINT: Use a for loop with the pandas iterrows() method\n",
    "# HINT: The iterrows() method returns two values: an index for each row and a tuple of values\n",
    "# HINT: Some of the values for totalamt and year are NaN. Because you've defined\n",
    "# year and totalamt as numbers, you cannot insert NaN as a value into those columns.\n",
    "# When totaamt or year equal NaN, you'll need to change the value to something numeric\n",
    "# like, for example, zero\n",
    "\n",
    "for index, values in df_projects.iterrows():\n",
    "    project_id, countryname, countrycode, totalamt, year = values\n",
    "    \n",
    "    if totalamt == 'nan':\n",
    "        totalamt = 0\n",
    "    if year == 'nan':\n",
    "        year = 0\n",
    "    \n",
    "    sql_string = 'INSERT INTO projects (project_id, countryname, countrycode, totalamt, year) VALUES (\"{}\", \"{}\", \"{}\", {}, {});'.format(project_id, countryname, countrycode, totalamt, year)\n",
    "    cur.execute(sql_string)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: insert gdp values into the gdp table\n",
    "for index, values in df_indicator[['countryname', 'countrycode', 'year', 'gdp']].iterrows():\n",
    "    countryname, countrycode, year, gdp = values\n",
    "        \n",
    "    sql_string = 'INSERT INTO gdp (countryname, countrycode, year, gdp) VALUES (\"{}\", \"{}\", {}, {});'.format(countryname, countrycode, year, gdp)\n",
    "    cur.execute(sql_string)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: insert population values into the population table\n",
    "for index, values in df_indicator[['countryname', 'countrycode', 'year', 'population']].iterrows():\n",
    "    countryname, countrycode, year, population = values\n",
    "        \n",
    "    sql_string = 'INSERT INTO population (countryname, countrycode, year, population) VALUES (\"{}\", \"{}\", {}, {});'.format(countryname, countrycode, year, population)\n",
    "    cur.execute(sql_string)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this command to see if your tables were loaded as expected\n",
    "sqlquery = \"SELECT * FROM projects JOIN gdp JOIN population ON projects.year = gdp.year AND projects.countrycode = gdp.countrycode AND projects.countrycode = population.countrycode AND projects.year=population.year;\"\n",
    "result = pd.read_sql(sqlquery, con=conn)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commit any changes and close the database\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
